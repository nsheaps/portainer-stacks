# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json
name: 'heapsnas'

# Set in env on portainer:
# - GITHUB_TOKEN = <pat portainer uses>
# - GITHUB_REPO_OWNER = nsheaps
# - GITHUB_REPO_NAME = portainer-stacks
# - OP_SERVICE_ACCOUNT_TOKEN

x-shared: &shared
  restart: always
  image: docker.n8n.io/n8nio/n8n:1.115.1
  environment:
    # ref: https://docs.n8n.io/hosting/configuration/environment-variables/
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=postgres
    - DB_POSTGRESDB_PORT=5432
    - DB_POSTGRESDB_DATABASE_FILE=/run/secrets/postgres_db
    - DB_POSTGRESDB_USER_FILE=/run/secrets/postgres_user
    - DB_POSTGRESDB_PASSWORD_FILE=/run/secrets/postgres_password
    - EXECUTIONS_MODE=queue
    - QUEUE_BULL_REDIS_HOST=redis
    - QUEUE_HEALTH_CHECK_ACTIVE=true
    - N8N_ENCRYPTION_KEY_FILE=/run/secrets/n8n_encryption_key
    - N8N_RUNNERS_ENABLED=true
    - N8N_EDITOR_BASE_URL=https://n8n.heaps.cloud
    - # https://docs.n8n.io/hosting/configuration/configuration-examples/webhook-url/
      WEBHOOK_URL=https://n8n.heaps.cloud/
    - N8N_PROXY_HOPS=1
  links:
    - postgres
    - redis
  volumes:
    - n8n_storage:/home/node/.n8n
    - secrets_volume:/run/secrets:ro
  depends_on:
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
    secrets-init:
      condition: service_completed_successfully
      # we don't want to restart every time this is
      # restarted, since every update restarts this,
      # which would cause downtime.
      # restart: true
services:
  # GITHUB_TOKEN, GITHUB_OWNER, and GITHUB_REPO should be set in Portainer as stack environment variable
  # Portainer-ce doesn't support mounting from the local directory.
  # If using agent, we'd need to download the repo from git like this
  # https://github.com/portainer/portainer/issues/6390#issuecomment-1100954657
  # In theory we could change portainer's volume to persist to disk, but that doesn't work
  # for agents (where the agent runs the stack instead of the root instance, not typical of
  # docker-compose setups
  git-checkout:
    image: alpine/git:latest
    working_dir: /tmp
    entrypoint: ['/bin/sh']
    command:
      - -c
      - |
        set -euo pipefail
        set -x
        cd /mnt/portainer-stacks
        echo if the git repository has already been initialized
        echo fetch and reset to the latest HEAD on the remote default
        echo branch
        START_TIME=$(date +%s)
        if [ -d .git ]; then
          echo "Fetching and resetting to HEAD of default branch on remote..."
          git fetch origin
          git reset --hard origin/$(git rev-parse --abbrev-ref HEAD)
        else
          # otherwise, check it out - note stores credentials in .git via the remote origin url
          git clone https://${GITHUB_TOKEN}@github.com/${GITHUB_OWNER}/${GITHUB_REPO}.git .
        fi
        rm -rf /mnt/postgres_init/*
        cp -R /mnt/portainer-stacks/hosts/heapsnas/volumes/postgres_init /mnt/postgres_init
        END_TIME=$(date +%s)
        ELAPSED_TIME=$((END_TIME - START_TIME))
        echo "took: $${ELAPSED_TIME} seconds"
    restart: on-failure
    volumes:
      - portainer_stacks:/mnt/portainer-stacks
      - postgres_init:/mnt/postgres_init

  # Init container to fetch secrets from 1Password
  secrets-init:
    image: 1password/op:2
    user: '0:0' # Run as root for putting in the secrets dir
    environment:
      # OP_SERVICE_ACCOUNT_TOKEN should be set in Portainer as stack environment variable
      - OP_SERVICE_ACCOUNT_TOKEN=${OP_SERVICE_ACCOUNT_TOKEN}
    volumes:
      - secrets_volume:/run/secrets:rw
      - portainer_stacks:/mnt/portainer-stacks
    depends_on:
      git-checkout:
        condition: service_completed_successfully
        restart: true
    command:
      [
        '/bin/bash',
        '-c',
        '/mnt/portainer-stacks/hosts/heapsnas/scripts/fetch-secrets.sh || { sleep 300; exit 1; }',
      ]

  cloudflared:
    image: cloudflare/cloudflared:2025.9.1
    restart: always
    command: tunnel --no-autoupdate run --token-file /run/secrets/cloudflared_token
    volumes:
      - secrets_volume:/run/secrets:ro
    depends_on:
      secrets-init:
        condition: service_completed_successfully

  postgres:
    image: postgres:18
    restart: always
    environment:
      - POSTGRES_USER_FILE=/run/secrets/postgres_root_user
      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_root_password
      - # We don't want to use the n8n one here because the init job takes care of it
        N8N_POSTGRES_DB_FILE=/run/secrets/postgres_db
        POSTGRES_DB=postgres
    volumes:
      - db_storage:/var/lib/postgresql/data
      - secrets_volume:/run/secrets:ro
      - portainer_stacks:/mnt/portainer-stacks
      - postgres_init:/docker-entrypoint-initdb.d:ro
      # - type: volume
      #   source: portainer_stacks
      #   target: /docker-entrypoint-initdb.d/init-postgres.sh
      #   read_only: true
      #   volume:
      #     subpath: hosts/heapsnas/scripts/init-postgres.sh
    healthcheck:
      test:
        [
          'CMD-SHELL',
          'pg_isready -h localhost -U $(cat $${POSTGRES_USER_FILE}) -d $(cat $${N8N_POSTGRES_DB_FILE:-does_not_exist})',
        ]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      secrets-init:
        condition: service_completed_successfully
        # we don't want to restart every time this is
        # restarted, since every update restarts this, which would cause
        # downtime.
        # restart: true

  redis:
    image: redis:8-alpine
    restart: always
    volumes:
      - redis_storage:/data
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
      interval: 5s
      timeout: 5s
      retries: 10

  n8n:
    <<: *shared
    ports:
      - '${N8N_PORT:-5678}:5678'

  n8n-worker:
    <<: *shared
    command: worker
    depends_on:
      - n8n

volumes:
  portainer_stacks:
    driver: local
  postgres_init:
    driver: local
  db_storage:
    driver: local
  n8n_storage:
    driver: local
  redis_storage:
    driver: local
  secrets_volume:
    # can't use tmpfs because we need to share between multiple containers
    # https://docs.docker.com/engine/storage/tmpfs/#limitations-of-tmpfs-mounts:~:text=Unlike%20volumes%20and%20bind%20mounts%2C%20you%20can%27t%20share%20tmpfs%20mounts%20between%20containers.
    driver: local
